{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ff237a45",
      "metadata": {
        "id": "title_cell"
      },
      "source": [
        "# MARS6 Turbo Inference Demo\n",
        "\n",
        "This notebook demonstrates how to run text-to-speech (TTS) inference using **MARS6** in two modes:\n",
        "- **Shallow clone**: only clones the speaker's voice timbre.\n",
        "- **Deep clone**: also uses the reference transcript and reference tokens to better match source reference, particularly in prosody.\n",
        "\n",
        "The notebook is broken into steps:\n",
        "1. **Import libraries**\n",
        "2. **Set device**\n",
        "3. **(Opt.1) Load model from local directory**\n",
        "4. **(Opt.2) Load model from Torch Hub**\n",
        "5. **Load embeddings / SNAC / config**\n",
        "6. **Generate reference embeddings** (one-time, or re-run with a different audio path if wanting to use a different reference)\n",
        "7. **Define the final inference logic** (assumes globally accessible reference embeddings)\n",
        "8. **Shallow clone**\n",
        "9. **Deep clone**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "836cc14b",
      "metadata": {
        "id": "step1_imports"
      },
      "source": [
        "## 1) Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c68900e9",
      "metadata": {
        "id": "cell_imports"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, List\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "import librosa\n",
        "\n",
        "from mars6_turbo.ar_model import Mars6_Turbo, SNACTokenizerInfo, RASConfig\n",
        "from mars6_turbo.minbpe.regex import RegexTokenizer\n",
        "from mars6_turbo.utils import RegexSplitter\n",
        "from snac import SNAC\n",
        "from transformers import Wav2Vec2FeatureExtractor, WavLMForXVector\n",
        "from msclap import CLAP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "266a1578",
      "metadata": {
        "id": "step2_device"
      },
      "source": [
        "## 2) Set device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63cb6c12",
      "metadata": {
        "id": "cell_device"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "dtype = torch.half if device == 'cuda' else torch.float\n",
        "print(f\"Using device={device}, dtype={dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a331ca5a",
      "metadata": {
        "id": "step3_localmodel"
      },
      "source": [
        "## 3) (Opt.1) Load model from local directory\n",
        "If you have a local checkpoint (e.g. `model/model-2000100.pt`), load it here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a11af1c4",
      "metadata": {
        "id": "cell_localmodel"
      },
      "outputs": [],
      "source": [
        "# Example local paths:\n",
        "ckpt_path = \"model/model-2000100.pt\"\n",
        "tokenizer_path = \"model/eng-tok-512.model\"\n",
        "\n",
        "# Load text tokenizer\n",
        "texttok = RegexTokenizer()\n",
        "texttok.load(tokenizer_path)\n",
        "print(\"Local tokenizer loaded.\")\n",
        "\n",
        "# Load checkpoint from disk\n",
        "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
        "model_cfg = ckpt['cfg']\n",
        "old_sd = ckpt['model']\n",
        "# Remove any 'module.' if present\n",
        "new_sd = {}\n",
        "for k,v in old_sd.items():\n",
        "    new_sd[k.replace('module.', '')] = v\n",
        "\n",
        "# Build MARS6 model\n",
        "n_text_vocab = len(texttok.vocab)\n",
        "n_speech_vocab = SNACTokenizerInfo.codebook_size*3 + SNACTokenizerInfo.n_snac_special\n",
        "\n",
        "model = Mars6_Turbo(\n",
        "    n_input_vocab=n_text_vocab,\n",
        "    n_output_vocab=n_speech_vocab,\n",
        "    emb_dim=model_cfg.get('dim', 512),\n",
        "    n_layers=model_cfg.get('n_layers', 8),\n",
        "    fast_n_layers=model_cfg.get('fast_n_layers', 4),\n",
        "    n_langs=len(model_cfg.get('languages', ['en-us']))\n",
        ")\n",
        "model.load_state_dict(new_sd)\n",
        "model = model.to(device=device, dtype=dtype)\n",
        "model.eval()\n",
        "print(\"Local MARS6 model loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b7d4f98",
      "metadata": {
        "id": "step4_hub"
      },
      "source": [
        "## 4) (Opt.2) Load model from Torch Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21b5d742",
      "metadata": {},
      "outputs": [],
      "source": [
        "model, texttok = torch.hub.load(\n",
        "    repo_or_dir=\"Camb-ai/mars6-turbo\",\n",
        "    model=\"mars6_turbo\",\n",
        "    ckpt_format='pt',\n",
        "    device=device,\n",
        "    dtype=dtype,\n",
        "    force_reload=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "534fd22b",
      "metadata": {
        "id": "step5_embeddings"
      },
      "source": [
        "## 5) Load Embeddings, SNAC Codec, and Default Config\n",
        "Here we load the speaker embedding models (CLAP + WavLM-based) and the SNAC codec. We also define a default config dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "600beb5e",
      "metadata": {
        "id": "cell_embeddings"
      },
      "outputs": [],
      "source": [
        "# Load embedding models\n",
        "clap_model = CLAP(use_cuda=(device=='cuda'))\n",
        "wavlm_feat_ext = Wav2Vec2FeatureExtractor.from_pretrained('microsoft/wavlm-base-plus-sv')\n",
        "spk_emb_model = WavLMForXVector.from_pretrained('microsoft/wavlm-base-plus-sv').to(device).eval()\n",
        "print(\"CLAP + WavLM speaker models loaded.\")\n",
        "\n",
        "# SNAC codec\n",
        "snac_codec = SNAC.from_pretrained('hubertsiuzdak/snac_24khz').eval().to(device, dtype)\n",
        "print(\"SNAC codec loaded.\")\n",
        "\n",
        "# Basic config\n",
        "config = {\n",
        "    'sr': 24000,\n",
        "    'ras_K': 10,\n",
        "    'ras_t_r': 0.09,\n",
        "    'top_p': 0.2,\n",
        "    'sil_trim_db': 33,\n",
        "    'backoff_top_p_increment': 0.2,\n",
        "    'chars_per_second_upper_bound': 32,\n",
        "    'min_valid_audio_volume': -52,\n",
        "    'prefix': '48000',\n",
        "    # can be 'none', 'per-chunk', or 'fixed-ref'\n",
        "    'deep_clone_mode': 'none'\n",
        "}\n",
        "print(\"Default config set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24e7ea87",
      "metadata": {},
      "source": [
        "## 6) Generate Reference Embeddings (One-time)\n",
        "Here we load and embed the reference audio just once. This yields:\n",
        "- A **CLAP** embedding: `clap_emb_global`\n",
        "- A **WavLM** speaker embedding: `spk_emb_global`\n",
        "- Optionally, **SNAC tokens** if we want to do *deep clone* by referencing the exact audio tokens: `ref_tokens_global`\n",
        "\n",
        "We can then pass these directly to `make_predictions(...)` in the shallow or deep clone steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "265db11e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_speech(\n",
        "    speechtok: SNACTokenizerInfo,\n",
        "    codes: List[torch.Tensor],\n",
        "    add_special: bool = True\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Convert each codebook array into offsets, flatten them, then shape to (sl,7).\n",
        "    (Used if you want to incorporate reference code tokens for deep clone.)\n",
        "    \"\"\"\n",
        "    tokens = []\n",
        "    # offset each codebook by i*4096 for i in {0,1,2}:\n",
        "    codes = [(c + (i * speechtok.codebook_size)).tolist() for i, c in enumerate(codes)]\n",
        "    quant_levels = [0, 1, 2]\n",
        "\n",
        "    while any(len(c) > 0 for c in codes):\n",
        "        for i in quant_levels:\n",
        "            if i == 0:\n",
        "                tokens.append(codes[0][0])\n",
        "                codes[0] = codes[0][1:]\n",
        "            elif i == 1:\n",
        "                tokens.extend(codes[1][:2])\n",
        "                codes[1] = codes[1][2:]\n",
        "            elif i == 2:\n",
        "                tokens.extend(codes[2][:4])\n",
        "                codes[2] = codes[2][4:]\n",
        "    if add_special:\n",
        "        # Append 7 eos tokens\n",
        "        tokens += [speechtok.eos_tok] * 7\n",
        "\n",
        "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "    tokens = tokens.view(-1, 7)  # shape (sl,7)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def snac_encode_reference(\n",
        "    snac_codec: SNAC,\n",
        "    wav: torch.Tensor,\n",
        "    device='cuda',\n",
        "    dtype=torch.float16\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Encode a waveform using SNAC, then flatten codebooks into (sl,7).\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        # shape: (1,T) => snac.encode expects B=1\n",
        "        wav = wav.to(device=device, dtype=dtype)\n",
        "        codes_list = snac_codec.encode(wav[None])\n",
        "        codes_list = [t.squeeze(0) for t in codes_list]\n",
        "        ref_tokens = tokenize_speech(SNACTokenizerInfo(), codes_list, add_special=True)\n",
        "    return ref_tokens\n",
        "\n",
        "def compute_ref_data(\n",
        "    audio_path: str,\n",
        "    sr_out=24000,\n",
        "    do_tokens: bool = True,\n",
        "    device: str = 'cuda',\n",
        "    dtype: torch.dtype = torch.half\n",
        "):\n",
        "    \"\"\"Load and process reference audio => CLAP embed, WavLM embed, optional SNAC tokens.\"\"\"\n",
        "    raw_wav, sr_in = torchaudio.load(audio_path)\n",
        "    raw_wav = raw_wav.mean(dim=0, keepdim=True)\n",
        "    if sr_in != sr_out:\n",
        "        raw_wav = torchaudio.functional.resample(raw_wav, sr_in, sr_out)\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(suffix='.wav', delete=True) as tmp:\n",
        "        torchaudio.save(tmp.name, raw_wav, sr_out)\n",
        "        clap_emb = clap_model.get_audio_embeddings([tmp.name], resample=True)[0].unsqueeze(0).to(device=device, dtype=dtype)\n",
        "\n",
        "    # WavLM spk embed at 16kHz\n",
        "    wav_16 = torchaudio.functional.resample(raw_wav, sr_out, 16000)\n",
        "    inp = wavlm_feat_ext(\n",
        "        wav_16.squeeze(),\n",
        "        padding=True,\n",
        "        return_tensors='pt',\n",
        "        sampling_rate=16000\n",
        "    )\n",
        "    for k in inp:\n",
        "        inp[k] = inp[k].to(device)\n",
        "    spk_emb = spk_emb_model(**inp).embeddings\n",
        "    spk_emb = torch.nn.functional.normalize(spk_emb, dim=-1).to(device=device, dtype=dtype)\n",
        "\n",
        "    ref_tokens = None\n",
        "    if do_tokens:\n",
        "        ref_tokens = snac_encode_reference(snac_codec, raw_wav, device=device, dtype=dtype)\n",
        "        ref_tokens = ref_tokens[:-2]\n",
        "\n",
        "    return spk_emb, clap_emb, ref_tokens\n",
        "\n",
        "print(\"Function to compute reference data (embedding + tokens) loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af3cfbea",
      "metadata": {},
      "outputs": [],
      "source": [
        "ref_audio_path = 'example.wav'\n",
        "spk_emb_global, clap_emb_global, ref_tokens_global = compute_ref_data(\n",
        "    audio_path=ref_audio_path,\n",
        "    sr_out=24000,\n",
        "    do_tokens=True,\n",
        "    device=device,\n",
        "    dtype=dtype\n",
        ")\n",
        "print(\"Reference embeddings + tokens computed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e485d8b7",
      "metadata": {
        "id": "step6_logic"
      },
      "source": [
        "## 7) Define the Inference Logic (Without Reference Embedding)\n",
        "Below we define an inference function that **assumes** you already have speaker embeddings (`spk_emb_global`), CLAP embeddings (`clap_emb_global`), and optionally reference tokens for deep clone (`ref_tokens_global`).\n",
        "We do **not** embed the reference audio inside this function; we handled that in Step 6.\n",
        "This is done to reduce computation when doing multiple TTS calls for the same reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0a847db",
      "metadata": {
        "id": "cell_inference_logic"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class EvalConfig:\n",
        "    sr: int = 24000\n",
        "    ras_K: int = 10\n",
        "    ras_t_r: float = 0.09\n",
        "    top_p: float = 0.2\n",
        "    sil_trim_db: float = 33\n",
        "    backoff_top_p_increment: float = 0.2\n",
        "    chars_per_second_upper_bound: float = 32\n",
        "    min_valid_audio_volume: float = -52\n",
        "    prefix: str = \"48000\"\n",
        "    # Options: 'none', 'per-chunk', or 'fixed-ref'\n",
        "    deep_clone_mode: str = 'per-chunk'\n",
        "\n",
        "# A simple punctuation mapping for unifying certain characters\n",
        "punctuation_mapping = {\n",
        "    \"。\": \".\", \"、\": \",\", \"！\": \"!\", \"？\": \"?\",\n",
        "    \"…\": \"...\", '–': '—', '―': '—', '−': '—', '-': '—', '।': '.'\n",
        "}\n",
        "punctuation_trans_table = str.maketrans(punctuation_mapping)\n",
        "\n",
        "\n",
        "def normalize_ref_volume(wav: torch.Tensor, sr: int, target_db: Optional[float]) -> tuple[torch.Tensor, Optional[float]]:\n",
        "    \"\"\"Normalize waveform loudness to a target dB using torchaudio.\"\"\"\n",
        "    if target_db is None:\n",
        "        return wav, None\n",
        "    ln = torchaudio.functional.loudness(wav, sr)\n",
        "    wav = torchaudio.functional.gain(wav, target_db - ln)\n",
        "    return wav, ln\n",
        "\n",
        "\n",
        "def detokenize_speech(codes: torch.Tensor) -> List[torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Convert model output tokens (shape (sl,7)) back into separate lists\n",
        "    of L0, L1, L2 token IDs, removing <eos>.\n",
        "    \"\"\"\n",
        "    eos_inds = codes.max(dim=-1).values == SNACTokenizerInfo.eos_tok\n",
        "    codes = codes[~eos_inds]\n",
        "    # revert L1 offsets\n",
        "    codes[:, 1:] -= SNACTokenizerInfo.codebook_size\n",
        "    # revert L2 offsets\n",
        "    codes[:, 3:] -= SNACTokenizerInfo.codebook_size\n",
        "    l0 = codes[:, 0]\n",
        "    l1 = codes[:, 1:3].flatten()\n",
        "    l2 = codes[:, 3:].flatten()\n",
        "    return [l0, l1, l2]\n",
        "\n",
        "\n",
        "def codes2duration(codes: List[torch.Tensor]) -> float:\n",
        "    \"\"\"\n",
        "    Approximate audio duration from hierarchical SNAC tokens.\n",
        "    L2 is at 48 Hz, so each token is 1/48 seconds.\n",
        "    \"\"\"\n",
        "    l2 = codes[2]\n",
        "    return len(l2) / 48.0\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "def make_predictions(\n",
        "    model: Mars6_Turbo,\n",
        "    texttok: RegexTokenizer,\n",
        "    cfg: dict,\n",
        "    snac_codec: SNAC,\n",
        "    text: str,\n",
        "    ref_text: str,\n",
        "    device: str = 'cuda',\n",
        "):\n",
        "    \"\"\"\n",
        "    End-to-end TTS inference that supports:\n",
        "      - Shallow clone (deep_clone_mode='none')\n",
        "      - Fixed-ref deep clone (deep_clone_mode='fixed-ref')\n",
        "      - Per-chunk deep clone (deep_clone_mode='per-chunk')\n",
        "\n",
        "    Where \"deep clone\" means we incorporate code tokens from the reference\n",
        "    (or from the previously generated chunk) as a prefix, to better match prosody.\n",
        "    \"\"\"\n",
        "    # Create the config as an EvalConfig dataclass\n",
        "    config = EvalConfig(**cfg)\n",
        "    splitter = RegexSplitter()\n",
        "    loudness_tfm = torchaudio.transforms.Loudness(config.sr)\n",
        "\n",
        "    # 1) Prepare for potential deep clone\n",
        "    deep_mode = config.deep_clone_mode\n",
        "    user_ref_text = ref_text.strip().translate(punctuation_trans_table)\n",
        "\n",
        "    # If user gives a short, but non-zero transcript or one that is too long => skip the first chunk of per-chunk cloning\n",
        "    if (0 < len(user_ref_text) < 4 or len(user_ref_text) > 300):\n",
        "        print(f\"Invalid transcript length={len(user_ref_text)}, disabling deep clone.\")\n",
        "        deep_mode = 'per-chunk'\n",
        "        user_ref_text = \"\"\n",
        "\n",
        "    # We'll store for per-chunk usage\n",
        "    prior_chunk_text = user_ref_text\n",
        "    if prior_chunk_text is not \"\":\n",
        "        prior_chunk_tokens = ref_tokens_global\n",
        "    else:\n",
        "        prior_chunk_tokens = None\n",
        "\n",
        "    # 2) Prepare text to generate, then chunk it\n",
        "    text_to_speak = text.translate(punctuation_trans_table)\n",
        "    chunks = splitter.split(text_to_speak)\n",
        "    if not chunks:\n",
        "        chunks = [text_to_speak]  # fallback if no splits\n",
        "    all_chunk_wavs = []\n",
        "    chunk_counter = 0\n",
        "\n",
        "    # 3) Generate chunk by chunk\n",
        "    for chunk_text in chunks:\n",
        "        chunk_text = chunk_text.strip()\n",
        "        if not chunk_text:\n",
        "            continue\n",
        "\n",
        "        # Decide prefix tokens based on deep_clone_mode\n",
        "        if deep_mode == 'fixed-ref':\n",
        "            prefix_text = user_ref_text\n",
        "            prefix_tokens = ref_tokens_global\n",
        "        elif deep_mode == 'per-chunk':\n",
        "            if chunk_counter == 0 and len(user_ref_text.strip())>0:\n",
        "                prefix_text = user_ref_text\n",
        "                prefix_tokens = ref_tokens_global\n",
        "            else:\n",
        "                prefix_text = prior_chunk_text\n",
        "                prefix_tokens = prior_chunk_tokens\n",
        "        else:\n",
        "            prefix_text = None\n",
        "            prefix_tokens = None\n",
        "\n",
        "        # Build text for the encoder\n",
        "        if prefix_text:\n",
        "            # E.g. \"[48000] reference_text + chunk_text\"\n",
        "            full_enc_str = f\"<|startoftext|>[{config.prefix}]{prefix_text} {chunk_text}<|endoftext|>\"\n",
        "        else:\n",
        "            full_enc_str = f\"<|startoftext|>[{config.prefix}]{chunk_text}<|endoftext|>\"\n",
        "\n",
        "        text_ids = texttok.encode(full_enc_str, allowed_special=\"all\")\n",
        "        x = torch.tensor(text_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
        "        xlengths = torch.tensor([x.shape[1]], dtype=torch.long, device=device)\n",
        "\n",
        "        language = torch.tensor([0], dtype=torch.long, device=device)\n",
        "\n",
        "        # We'll ensure minimal chunk duration\n",
        "        chunk_dur_min = len(chunk_text) / config.chars_per_second_upper_bound\n",
        "        chunk_wav_final = None\n",
        "\n",
        "        tries = 0\n",
        "        while True:\n",
        "            tries += 1\n",
        "            ras_cfg = RASConfig(\n",
        "                K=config.ras_K,\n",
        "                t_r=config.ras_t_r,\n",
        "                top_p=config.top_p,\n",
        "                enabled=True,\n",
        "                cfg_guidance=1.0\n",
        "            )\n",
        "\n",
        "            max_len = 30 + int(xlengths.item() * 2.6)\n",
        "\n",
        "            result_tokens = model.inference(\n",
        "                x,\n",
        "                xlengths,\n",
        "                clap_embs=clap_emb_global,\n",
        "                spk_embs=spk_emb_global,\n",
        "                language=language,\n",
        "                max_len=max_len,\n",
        "                fp16=(device == 'cuda'),\n",
        "                ras_cfg=ras_cfg,\n",
        "                cache=None,\n",
        "                decoder_prefix=prefix_tokens,  # deep clone prefix\n",
        "                lower_bound_dur=chunk_dur_min\n",
        "            )\n",
        "\n",
        "            # Detokenize => approximate duration\n",
        "            rcodes = detokenize_speech(result_tokens)\n",
        "            chunk_dur = codes2duration(rcodes)\n",
        "\n",
        "            if chunk_dur < chunk_dur_min:\n",
        "                # If too short, we can do top_p fallback\n",
        "                new_top_p = round(min(config.top_p + config.backoff_top_p_increment, 1.0), 2)\n",
        "                print(f\"Chunk too short ({chunk_dur:.2f}s < {chunk_dur_min:.2f}s).\"\n",
        "                            f\" Increase top_p from {config.top_p} -> {new_top_p}. Retrying.\")\n",
        "                config.top_p = new_top_p\n",
        "                if tries > 10:\n",
        "                    print(\"Max tries reached for chunk. Using best so far.\")\n",
        "                    break\n",
        "                continue\n",
        "\n",
        "            # decode audio\n",
        "            chunk_audio = snac_codec.decode([r[None].to(device) for r in rcodes])\n",
        "            # shape => (1, batch?), we index the code dimension\n",
        "            chunk_audio = chunk_audio[:, 0]\n",
        "            loudness_val = loudness_tfm(chunk_audio.cpu().float().contiguous())\n",
        "            if loudness_val < config.min_valid_audio_volume:\n",
        "                # fallback again\n",
        "                new_top_p = round(min(config.top_p + config.backoff_top_p_increment, 1.0), 2)\n",
        "                print(f\"Chunk silent or quiet (loud={loudness_val:.2f}). \"\n",
        "                            f\"Increasing top_p to {new_top_p}.\")\n",
        "                config.top_p = new_top_p\n",
        "                if tries > 10:\n",
        "                    print(\"Max tries reached for chunk. Using best so far.\")\n",
        "                    break\n",
        "                continue\n",
        "\n",
        "            # We are done with this chunk\n",
        "            chunk_wav_final = chunk_audio.cpu().squeeze()\n",
        "            break\n",
        "\n",
        "        if chunk_wav_final is None:\n",
        "            continue  # no valid chunk generated\n",
        "        all_chunk_wavs.append(chunk_wav_final)\n",
        "\n",
        "        # For per-chunk mode, store newly generated tokens as prefix\n",
        "        if deep_mode == 'per-chunk':\n",
        "            prior_chunk_text = chunk_text\n",
        "            prior_chunk_tokens = result_tokens\n",
        "\n",
        "        chunk_counter += 1\n",
        "\n",
        "    # 6) Concatenate all chunk waveforms\n",
        "    if not all_chunk_wavs:\n",
        "        print(\"No audio was generated.\")\n",
        "        final_wav = torch.zeros(16000)  # fallback\n",
        "    else:\n",
        "        final_wav = torch.cat([wav for wav in all_chunk_wavs], dim=-1)\n",
        "\n",
        "    # trim silence\n",
        "    final_np, _ = librosa.effects.trim(final_wav.numpy(), top_db=config.sil_trim_db)\n",
        "    final_wav = torch.from_numpy(final_np)\n",
        "\n",
        "    return final_wav\n",
        "\n",
        "print(\"Inference logic defined (excluding reference embedding).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d57681c0",
      "metadata": {
        "id": "step8_shallow"
      },
      "source": [
        "## 8) Shallow Clone\n",
        "We already have `spk_emb_global` and `clap_emb_global` from step 6. For shallow clone, we pass `ref_tokens=None` to the inference function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0996a1cf",
      "metadata": {
        "id": "cell_shallow"
      },
      "outputs": [],
      "source": [
        "# config['deep_clone_mode'] = 'none'  # pure shallow clone\n",
        "\n",
        "config['deep_clone_mode'] = 'per-chunk'  # also capable of operating without a reference transcript...\n",
        "# ...typically has better flow over chunk boundaries (where chunk boundaries are at sentence splits)\n",
        "\n",
        "target_text = \"Hello from MARS six in shallow clone mode!\"\n",
        "reference_text = \"\"  # not used\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "out_wav = make_predictions(\n",
        "    model=model,\n",
        "    texttok=texttok,\n",
        "    cfg=config,\n",
        "    snac_codec=snac_codec,\n",
        "    text=target_text,\n",
        "    ref_text=reference_text,\n",
        "    device=device,\n",
        ")\n",
        "t_elapsed = time.time() - t0\n",
        "\n",
        "print(f\"Shallow clone done in {t_elapsed}s\")\n",
        "torchaudio.save(\"shallow_clone_output.wav\", out_wav.unsqueeze(0).float(), 24000)\n",
        "print(\"Saved to shallow_clone_output.wav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6f5656d",
      "metadata": {
        "id": "step9_deep"
      },
      "source": [
        "## 9) Deep Clone\n",
        "For deep clone, we provide the reference tokens from step 6 to the inference function, and optionally the reference text to combine in the encoder prefix. This can be done in `'fixed-ref'` or `'per-chunk'` mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ab79f8e",
      "metadata": {
        "id": "cell_deep"
      },
      "outputs": [],
      "source": [
        "config['deep_clone_mode'] = 'per-chunk'\n",
        "target_text = \"Now you can hear the same voice, with matched prosody and better similarity!\"\n",
        "reference_text = \"\"  # from reference audio\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "out_wav = make_predictions(\n",
        "    model=model,\n",
        "    texttok=texttok,\n",
        "    cfg=config,\n",
        "    snac_codec=snac_codec,\n",
        "    text=target_text,\n",
        "    ref_text=reference_text,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "t_elapsed = time.time() - t0\n",
        "print(f\"Deep clone done in {t_elapsed:.2f}s\")\n",
        "torchaudio.save(\"deep_clone_output.wav\", out_wav.unsqueeze(0).float(), 24000)\n",
        "print(\"Saved to deep_clone_output.wav\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "marstest2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "name": "mars6_turbo_inference_demo"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
